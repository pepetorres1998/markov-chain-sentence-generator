This paper presents a parallel algorithm for (MC)3. The proposed parallel algorithm retains the ability to explore multiple peaks in the posterior distribution of trees while maintaining a fast execution time. The algorithm has been implemented using two popular parallel programming models: message passing and shared memory. Performance results indicate nearly linear speed improvement in both programming models for small and large data sets. Bayesian estimation of phylogeny is based on the posterior probability distribution of trees. Currently, the only numerical method that can effectively approximate posterior probabilities of trees is Markov chain Monte Carlo (MCMC). Standard implementations of MCMC can be prone to entrapment in local optima. Metropolis coupled MCMC [(MC)3], a variant of MCMC, allows multiple peaks in the landscape of trees to be more readily explored, but at the cost of increased execution time. Genetic Programming (GP) homologous crossovers are a group of operators, including GP one-point crossover and GP uniform crossover, where the offspring are created preserving the position of the genetic material taken from the parents. In this paper we present an exact schema theory for GP and variable-length Genetic Algorithms (GAs) which is applicable to this class of operators. The theory is based on the concepts of GP crossover masks and GP recombination distributions that are generalisations of the corresponding notions used in GA theory and in population genetics, as well as the notions of hyperschema and node reference systems, which are specifically required when dealing with variable size representations. In this paper we also present a Markov chain model for GP and variable-length GAs with homologous crossover. We obtain this result by using the core of Vose's model for GAs in conjunction with the GP schema theory just described. The model is then specialised for the case of GP operating on 0/1 trees: a tree-like generalisation of the concept of binary string. For these, symmetries exist that can be exploited to obtain further simplifications. In the absence of mutation, the Markov chain model presented here generalises Vose's GA model to GP and variable-length GAs. Likewise, our schema theory generalises and refines a variety of previous results in GP and GA theory. This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis—Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors. We consider a symmetric random walk on a connected graph, where each edge is labeled with the probability of transition between the two adjacent vertices. The associated Markov chain has a uniform equilibrium distribution; the rate of convergence to this distribution, i.e., the mixing rate of the Markov chain, is determined by the second largest eigenvalue modulus (SLEM) of the transition probability matrix. In this paper we address the problem of assigning probabilities to the edges of the graph in such a way as to minimize the SLEM, i.e., the problem of finding the fastest mixing Markov chain on the graph. We show that this problem can be formulated as a convex optimization problem, which can in turn be expressed as a semidefinite program (SDP). This allows us to easily compute the (globally) fastest mixing Markov chain for any graph with a modest number of edges (say, $1000$) using standard numerical methods for SDPs. Larger problems can be solved by exploiting various types of symmetry and structure in the problem, and far larger problems (say, 100,000 edges) can be solved using a subgradient method we describe. We compare the fastest mixing Markov chain to those obtained using two commonly used heuristics: the maximum-degree method, and the Metropolis--Hastings algorithm. For many of the examples considered, the fastest mixing Markov chain is substantially faster than those obtained using these heuristic methods. We derive the Lagrange dual of the fastest mixing Markov chain problem, which gives a sophisticated method for obtaining (arbitrarily good) bounds on the optimal mixing rate, as well as the optimality conditions. Finally, we describe various extensions of the method, including a solution of the problem of finding the fastest mixing reversible Markov chain, on a fixed graph, with a given equilibrium distribution. Fans of the A Song Of Ice And Fire novels and the TV adaptation A Game Of Thrones know that horses appear frequently in the series. Some people assume that when I wrote these books, I must have had at least a passing knowledge of horses, but the truth is that when I began, I’d never seen a horse in my life. In fact, I had absolutely no idea what horses even looked like. I knew horses were animals of some kind, but beyond that, my entire conception of the way these animals actually looked was based on hearsay and my own speculation. Of course, I always wrote around my ignorance as much as possible. If you look back at A Clash Of Kings, you can see that I note in one section that “there were some large, normal-looking horses outside the main gate” and that I actually make repeated mentions later in the book of people riding around on “horse-sized horses.” I assumed that by just consciously refraining from going into specifics about a horse’s color, shape, or disposition, I could get by without anyone noticing. Inevitably, though, there were some details I got plain wrong. Tywin Lannister’s horse’s “great gleaming wheels,” the stallions outside Storm’s End “growling and barking in agitation” and the many passages describing Dothraki horses “flapping across the fields” seemed like natural descriptions at the time, but in retrospect I see how they could have betrayed my unfamiliarity with the creatures. For example, had I known how thick a real horse’s neck is, I would have realized how unrealistic it was to have Joffrey, a mere child, strangle a horse using only two fingers. And it still makes me somewhat cringe to read passages like the following: When Brienne turned she noticed Podrick growing weary atop his horse’s shell, tightly clutching its long, pointed horns to avoid falling. “My lady,” the boy quavered. “May we stop for a moment?” Then Podrick put the horse in his satchel and sat down to rest. But you learn as you write, and life makes you wiser as well. I remember it was midway through writing A Dance With Dragons when I saw a picture of a horse on a billboard. It was a revelation. Something suddenly clicked in my mind: “Oh, okay,” I thought. “Here is a horse. It doesn’t have a beak. This is what to imagine when I write about horses.” Since then, I’ve learned even more. My new knowledge has, I think, benefited the series terrifically. I’m still no zoologist, but I know enough to understand that horses’ bodies are covered in hair, that they have tails, and that no horse would ever be confused with a man, despite what Roose Bolton thought. Overall, I’ve grown as an author, and I hope readers will notice the difference.
